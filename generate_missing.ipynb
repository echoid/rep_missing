{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "#from ucimlrepo import fetch_ucirepo \n",
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "#import torch\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from scipy import optimize\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(suppress=True, precision=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataname_list = [\"banknote\",\"yeast\",\"climate_model_crashes\",\n",
    "                 \"wine_quality_white\", \"yacht_hydrodynamics\",\"concrete_compression\",\n",
    "                 \"breast_cancer\",\"solar_fire\",\"car_evluation\"\n",
    "                 ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_split_index_cv(scaled_data, directory_path, seed=1, nfold=5):\n",
    "    indlist = np.arange(len(scaled_data))\n",
    "\n",
    "    np.random.seed(seed)\n",
    "    np.random.shuffle(indlist)\n",
    "\n",
    "    fold_size = len(scaled_data) // nfold\n",
    "    save_index = {}\n",
    "\n",
    "    for fold in range(nfold):\n",
    "        start = fold * fold_size\n",
    "        end = start + fold_size if fold < nfold - 1 else len(scaled_data)\n",
    "        \n",
    "        test_index = indlist[start:end]\n",
    "        train_index = np.concatenate([indlist[:start], indlist[end:]])\n",
    "\n",
    "        # If you want to split the training set into train and validation sets\n",
    "        num_train = int(len(train_index) * 0.9)\n",
    "        train_subindex = train_index[:num_train]\n",
    "        valid_subindex = train_index[num_train:]\n",
    "\n",
    "        fold_index = {\n",
    "            \"test_index\": test_index.astype(np.int64).tolist(),\n",
    "            \"train_index\": train_subindex.astype(np.int64).tolist(),\n",
    "            \"valid_index\": valid_subindex.astype(np.int64).tolist()\n",
    "        }\n",
    "        save_index[f\"fold_{fold+1}\"] = fold_index\n",
    "\n",
    "    with open(f\"data/{directory_path}/split_index_cv_seed-{seed}_nfold-{nfold}.json\", 'w') as file:\n",
    "        json.dump(save_index, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(name):\n",
    "    if name == \"banknote\":\n",
    "        with open('data/banknote/data_banknote_authentication.txt', 'rb') as f:\n",
    "            df = pd.read_csv(f, low_memory=False, sep=',',header = None)\n",
    "            Xy = {}\n",
    "            # Ignore the two blocking factor\n",
    "            Xy['data'] = df.values[:, :-1]\n",
    "            Xy['target'] =  df.values[:, -1]\n",
    "    elif name == \"yeast\":\n",
    "        with open('data/yeast/yeast.data', 'rb') as f:\n",
    "            df = pd.read_csv(f, delimiter='\\s+', header = None)\n",
    "            Xy = {}\n",
    "            # remove index\n",
    "            Xy['data'] = df.values[:, 1:-1].astype('float')\n",
    "            Xy['target'] =  df.values[:, -1]\n",
    "    elif name == \"climate_model_crashes\":\n",
    "        with open('data/climate_model_crashes/pop_failures.dat', 'rb') as f:\n",
    "            df = pd.read_csv(f, delimiter='\\s+', header = 0)\n",
    "            Xy = {}\n",
    "            # Ignore the two blocking factor\n",
    "            Xy['data'] = df.values[:, 2:-1]\n",
    "            Xy['target'] =  df.values[:, -1]\n",
    "    elif name == \"wine_quality_white\":\n",
    "        with open('data/wine_quality_white/data.csv', 'rb') as f:\n",
    "            df = pd.read_csv(f, delimiter=';')\n",
    "            Xy = {}\n",
    "            Xy['data'] = df.values[:, :-1].astype('float')\n",
    "            Xy['target'] =  df.values[:, -1]\n",
    "\n",
    "    elif name == \"yacht_hydrodynamics\":\n",
    "        with open('data/yacht_hydrodynamics/yacht_hydrodynamics.data', 'rb') as f:\n",
    "            df = pd.read_csv(f, delimiter='\\s+', header = None)\n",
    "            Xy = {}\n",
    "            Xy['data'] = df.values[:, :-1]\n",
    "            Xy['target'] =  df.values[:, -1]\n",
    "\n",
    "    elif name == \"concrete_compression\":\n",
    "        with open('data/concrete_compression/Concrete_Data.xls', 'rb') as f:\n",
    "            df = pd.read_excel(io=f)\n",
    "            Xy = {}\n",
    "            Xy['data'] = df.values[:, :-1]\n",
    "            Xy['target'] =  df.values[:, -1]\n",
    "\n",
    "    elif name == \"breast_cancer\":\n",
    "        with open('data/breast_cancer/breast_cancer.data', 'rb') as f:\n",
    "            df = pd.read_csv(f, delimiter=',', header = None)\n",
    "            Xy = {}\n",
    "            Xy['data'] = df.values[:, :-1]\n",
    "            Xy['target'] =  df.values[:, -1]\n",
    "\n",
    "    elif name == \"solar_fire\":\n",
    "        with open('data/solar_fire/flare.data1', 'rb') as f:\n",
    "            df1 = pd.read_csv(f, delimiter='\\s+', header = None)\n",
    "        with open('data/solar_fire/flare.data2', 'rb') as f:\n",
    "            df2 = pd.read_csv(f, delimiter='\\s+', header = None)\n",
    "            df = pd.concat([df1, df2], ignore_index=True)\n",
    "            Xy = {}\n",
    "            Xy['data'] = df.values[:, :-1]\n",
    "            Xy['target'] =  df.values[:, -1]\n",
    "\n",
    "    elif name == \"car_evaluation\":\n",
    "        with open('data/car_evaluation/car.data', 'rb') as f:\n",
    "            df = pd.read_csv(f, delimiter=',', header = None)\n",
    "            Xy = {}\n",
    "            Xy['data'] = df.values[:, :-1]\n",
    "            Xy['target'] =  df.values[:, -1]\n",
    "\n",
    "    return Xy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "banknote 0.5\n",
      "[[0.769   0.83964 0.10678 0.73663]\n",
      " [0.83566 0.82098 0.1218  0.64433]\n",
      " [0.78663 0.41665 0.31061 0.78695]\n",
      " ...\n",
      " [0.23739 0.01177 0.9856  0.52476]\n",
      " [0.25084 0.2017  0.76159 0.66067]\n",
      " [0.32453 0.49075 0.34335 0.88595]]\n",
      "[0 0 0 ... 1 1 1]\n",
      "mcar\n",
      "[0.5 0.5 0.5 0.5] 0.5\n",
      "\n",
      "mar\n",
      "\n",
      "mnar\n",
      "Missing Rate 0.5\n",
      "\n",
      "yeast 0.5\n",
      "[[0.52809 0.55172 0.32911 ... 0.      0.65753 0.22   ]\n",
      " [0.35955 0.62069 0.34177 ... 0.      0.72603 0.22   ]\n",
      " [0.59551 0.56322 0.35443 ... 0.      0.72603 0.22   ]\n",
      " ...\n",
      " [0.62921 0.50575 0.18987 ... 0.      0.76712 0.22   ]\n",
      " [0.35955 0.31034 0.49367 ... 0.      0.72603 0.39   ]\n",
      " [0.60674 0.47126 0.41772 ... 0.      0.72603 0.22   ]]\n",
      "[6 6 6 ... 4 7 0]\n",
      "mcar\n",
      "[0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5] 0.5\n",
      "\n",
      "mar\n",
      "\n",
      "mnar\n",
      "Missing Rate 0.5\n",
      "\n",
      "climate_model_crashes 0.5\n",
      "[[0.85967 0.92879 0.25242 ... 0.86054 0.79751 0.87016]\n",
      " [0.60637 0.45723 0.35932 ... 0.35698 0.43863 0.5123 ]\n",
      " [0.9984  0.37247 0.51773 ... 0.25066 0.28568 0.36582]\n",
      " ...\n",
      " [0.47877 0.94219 0.77031 ... 0.86937 0.46183 0.65295]\n",
      " [0.00739 0.77979 0.86882 ... 0.36523 0.20143 0.5366 ]\n",
      " [0.6084  0.02973 0.59883 ... 0.46513 0.76082 0.76264]]\n",
      "[0 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 0 1 1 1 1 0 1 1 1 1 1 1 1 1 0 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 0 1 1 1 1 1 1 0 0 1 1 1 1 1 1 0 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1\n",
      " 1 1 1 0 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 0 0 1 1 1 1 1 1 1 1 0 1 1 1 1 0 1 1 1 1 1 1 1 0 1 1 1 1 1 1 0 1\n",
      " 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 0 1 1 0 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 0\n",
      " 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1\n",
      " 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 0 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 0 1 1 1\n",
      " 1 1 1 1 1 1 1 0 1 1 1 1 1 1 0 1 1 1 0 1 1 1]\n",
      "mcar\n",
      "[0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5] 0.5\n",
      "\n",
      "mar\n",
      "\n",
      "mnar\n",
      "Missing Rate 0.5\n",
      "\n",
      "wine_quality_white 0.5\n",
      "[[0.30769 0.18627 0.21687 ... 0.25455 0.26744 0.12903]\n",
      " [0.24038 0.21569 0.20482 ... 0.52727 0.31395 0.24194]\n",
      " [0.41346 0.19608 0.24096 ... 0.49091 0.25581 0.33871]\n",
      " ...\n",
      " [0.25962 0.15686 0.11446 ... 0.24545 0.27907 0.22581]\n",
      " [0.16346 0.20588 0.18072 ... 0.56364 0.18605 0.77419]\n",
      " [0.21154 0.12745 0.22892 ... 0.49091 0.11628 0.6129 ]]\n",
      "[[0.5    ]\n",
      " [0.5    ]\n",
      " [0.5    ]\n",
      " ...\n",
      " [0.5    ]\n",
      " [0.66667]\n",
      " [0.5    ]]\n",
      "mcar\n",
      "[0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5] 0.5\n",
      "\n",
      "mar\n",
      "\n",
      "mnar\n",
      "Missing Rate 0.5\n",
      "\n",
      "yacht_hydrodynamics 0.5\n",
      "[[0.54    0.54286 0.55    0.46457 0.48352 0.     ]\n",
      " [0.54    0.54286 0.55    0.46457 0.48352 0.07692]\n",
      " [0.54    0.54286 0.55    0.46457 0.48352 0.15385]\n",
      " ...\n",
      " [0.54    1.      0.      0.55906 0.      0.84615]\n",
      " [0.54    1.      0.      0.55906 0.      0.92308]\n",
      " [0.54    1.      0.      0.55906 0.      1.     ]]\n",
      "[[0.0016 ]\n",
      " [0.00417]\n",
      " [0.00737]\n",
      " [0.01234]\n",
      " [0.01875]\n",
      " [0.029  ]\n",
      " [0.04166]\n",
      " [0.06009]\n",
      " [0.07979]\n",
      " [0.11456]\n",
      " [0.191  ]\n",
      " [0.32206]\n",
      " [0.5246 ]\n",
      " [0.79282]\n",
      " [0.00048]\n",
      " [0.00256]\n",
      " [0.00577]\n",
      " [0.01041]\n",
      " [0.01682]\n",
      " [0.02532]\n",
      " [0.03717]\n",
      " [0.05256]\n",
      " [0.07371]\n",
      " [0.11376]\n",
      " [0.19196]\n",
      " [0.33777]\n",
      " [0.56081]\n",
      " [0.82983]\n",
      " [0.00128]\n",
      " [0.00449]\n",
      " [0.00881]\n",
      " [0.01362]\n",
      " [0.02083]\n",
      " [0.03173]\n",
      " [0.04695]\n",
      " [0.0673 ]\n",
      " [0.08861]\n",
      " [0.13203]\n",
      " [0.20942]\n",
      " [0.34273]\n",
      " [0.53084]\n",
      " [0.80324]\n",
      " [0.00304]\n",
      " [0.00545]\n",
      " [0.01025]\n",
      " [0.01474]\n",
      " [0.02179]\n",
      " [0.03141]\n",
      " [0.04519]\n",
      " [0.06377]\n",
      " [0.083  ]\n",
      " [0.12851]\n",
      " [0.2059 ]\n",
      " [0.3445 ]\n",
      " [0.54414]\n",
      " [0.80676]\n",
      " [0.00304]\n",
      " [0.00545]\n",
      " [0.01025]\n",
      " [0.01474]\n",
      " [0.02179]\n",
      " [0.03141]\n",
      " [0.04519]\n",
      " [0.06377]\n",
      " [0.083  ]\n",
      " [0.12851]\n",
      " [0.2059 ]\n",
      " [0.3445 ]\n",
      " [0.54414]\n",
      " [0.80676]\n",
      " [0.00176]\n",
      " [0.00401]\n",
      " [0.00673]\n",
      " [0.0109 ]\n",
      " [0.0173 ]\n",
      " [0.0266 ]\n",
      " [0.03926]\n",
      " [0.0548 ]\n",
      " [0.07387]\n",
      " [0.10976]\n",
      " [0.18507]\n",
      " [0.3304 ]\n",
      " [0.55264]\n",
      " [0.86877]\n",
      " [0.00433]\n",
      " [0.00689]\n",
      " [0.01106]\n",
      " [0.01698]\n",
      " [0.025  ]\n",
      " [0.03557]\n",
      " [0.04935]\n",
      " [0.06537]\n",
      " [0.09309]\n",
      " [0.13251]\n",
      " [0.20494]\n",
      " [0.32687]\n",
      " [0.51803]\n",
      " [0.75757]\n",
      " [0.00304]\n",
      " [0.00593]\n",
      " [0.01009]\n",
      " [0.01538]\n",
      " [0.02163]\n",
      " [0.03157]\n",
      " [0.04647]\n",
      " [0.06954]\n",
      " [0.09261]\n",
      " [0.12867]\n",
      " [0.19452]\n",
      " [0.30716]\n",
      " [0.48197]\n",
      " [0.71094]\n",
      " [0.00224]\n",
      " [0.00497]\n",
      " [0.00865]\n",
      " [0.01362]\n",
      " [0.01971]\n",
      " [0.02804]\n",
      " [0.03974]\n",
      " [0.05512]\n",
      " [0.07723]\n",
      " [0.11793]\n",
      " [0.20429]\n",
      " [0.35219]\n",
      " [0.5709 ]\n",
      " [0.85018]\n",
      " [0.0016 ]\n",
      " [0.00369]\n",
      " [0.00769]\n",
      " [0.0125 ]\n",
      " [0.02035]\n",
      " [0.03124]\n",
      " [0.04599]\n",
      " [0.06618]\n",
      " [0.09534]\n",
      " [0.14517]\n",
      " [0.23906]\n",
      " [0.38648]\n",
      " [0.61064]\n",
      " [0.88816]\n",
      " [0.00096]\n",
      " [0.00272]\n",
      " [0.00625]\n",
      " [0.01106]\n",
      " [0.01811]\n",
      " [0.02916]\n",
      " [0.04422]\n",
      " [0.06585]\n",
      " [0.08652]\n",
      " [0.12594]\n",
      " [0.20349]\n",
      " [0.33664]\n",
      " [0.55392]\n",
      " [0.82935]\n",
      " [0.00112]\n",
      " [0.00401]\n",
      " [0.00785]\n",
      " [0.01314]\n",
      " [0.02035]\n",
      " [0.03028]\n",
      " [0.04278]\n",
      " [0.06009]\n",
      " [0.08909]\n",
      " [0.1402 ]\n",
      " [0.22801]\n",
      " [0.36917]\n",
      " [0.56802]\n",
      " [0.83288]\n",
      " [0.00112]\n",
      " [0.00369]\n",
      " [0.00705]\n",
      " [0.01218]\n",
      " [0.01891]\n",
      " [0.02804]\n",
      " [0.04134]\n",
      " [0.06153]\n",
      " [0.08428]\n",
      " [0.12386]\n",
      " [0.19853]\n",
      " [0.33488]\n",
      " [0.53229]\n",
      " [0.78721]\n",
      " [0.00112]\n",
      " [0.00385]\n",
      " [0.00721]\n",
      " [0.01186]\n",
      " [0.01763]\n",
      " [0.025  ]\n",
      " [0.03461]\n",
      " [0.04759]\n",
      " [0.07066]\n",
      " [0.12546]\n",
      " [0.22593]\n",
      " [0.38664]\n",
      " [0.60792]\n",
      " [0.88383]\n",
      " [0.00144]\n",
      " [0.00353]\n",
      " [0.00737]\n",
      " [0.01202]\n",
      " [0.01827]\n",
      " [0.02628]\n",
      " [0.03637]\n",
      " [0.04935]\n",
      " [0.0705 ]\n",
      " [0.12017]\n",
      " [0.22048]\n",
      " [0.38375]\n",
      " [0.59878]\n",
      " [0.9045 ]\n",
      " [0.00064]\n",
      " [0.00256]\n",
      " [0.00545]\n",
      " [0.00993]\n",
      " [0.01602]\n",
      " [0.02275]\n",
      " [0.03269]\n",
      " [0.04358]\n",
      " [0.06185]\n",
      " [0.11505]\n",
      " [0.22352]\n",
      " [0.4033 ]\n",
      " [0.66223]\n",
      " [1.     ]\n",
      " [0.00032]\n",
      " [0.00272]\n",
      " [0.00625]\n",
      " [0.01154]\n",
      " [0.02067]\n",
      " [0.03445]\n",
      " [0.05352]\n",
      " [0.08092]\n",
      " [0.11424]\n",
      " [0.16584]\n",
      " [0.24419]\n",
      " [0.37077]\n",
      " [0.55456]\n",
      " [0.82503]\n",
      " [0.0008 ]\n",
      " [0.00224]\n",
      " [0.00529]\n",
      " [0.00993]\n",
      " [0.01795]\n",
      " [0.02948]\n",
      " [0.04535]\n",
      " [0.06938]\n",
      " [0.09918]\n",
      " [0.13796]\n",
      " [0.19997]\n",
      " [0.32687]\n",
      " [0.51995]\n",
      " [0.81606]\n",
      " [0.0024 ]\n",
      " [0.00497]\n",
      " [0.00929]\n",
      " [0.01458]\n",
      " [0.02179]\n",
      " [0.03092]\n",
      " [0.04182]\n",
      " [0.05913]\n",
      " [0.08717]\n",
      " [0.15126]\n",
      " [0.26118]\n",
      " [0.43791]\n",
      " [0.66912]\n",
      " [0.97484]\n",
      " [0.00128]\n",
      " [0.00369]\n",
      " [0.00737]\n",
      " [0.01234]\n",
      " [0.01923]\n",
      " [0.02948]\n",
      " [0.04182]\n",
      " [0.05896]\n",
      " [0.08108]\n",
      " [0.12722]\n",
      " [0.21984]\n",
      " [0.37718]\n",
      " [0.59494]\n",
      " [0.89505]\n",
      " [0.     ]\n",
      " [0.0024 ]\n",
      " [0.00609]\n",
      " [0.01154]\n",
      " [0.01971]\n",
      " [0.03124]\n",
      " [0.04855]\n",
      " [0.0713 ]\n",
      " [0.10095]\n",
      " [0.13892]\n",
      " [0.19837]\n",
      " [0.32254]\n",
      " [0.50889]\n",
      " [0.75501]\n",
      " [0.00048]\n",
      " [0.00256]\n",
      " [0.00561]\n",
      " [0.01009]\n",
      " [0.01618]\n",
      " [0.0258 ]\n",
      " [0.04198]\n",
      " [0.06634]\n",
      " [0.09598]\n",
      " [0.13556]\n",
      " [0.19644]\n",
      " [0.31373]\n",
      " [0.48822]\n",
      " [0.74748]]\n",
      "mcar\n",
      "[0.5 0.5 0.5 0.5 0.5 0.5] 0.5\n",
      "\n",
      "mar\n",
      "\n",
      "mnar\n",
      "Missing Rate 0.5\n",
      "\n",
      "concrete_compression 0.5\n",
      "[[1.      0.      0.      ... 0.69477 0.20572 0.07418]\n",
      " [1.      0.      0.      ... 0.73837 0.20572 0.07418]\n",
      " [0.52626 0.39649 0.      ... 0.38081 0.      0.73901]\n",
      " ...\n",
      " [0.10616 0.38787 0.54273 ... 0.2657  0.46663 0.07418]\n",
      " [0.13037 0.51948 0.      ... 0.54826 0.48896 0.07418]\n",
      " [0.36279 0.27963 0.3913  ... 0.18459 0.42022 0.07418]]\n",
      "[[0.96744]\n",
      " [0.74196]\n",
      " [0.47264]\n",
      " ...\n",
      " [0.26617]\n",
      " [0.37919]\n",
      " [0.37462]]\n",
      "mcar\n",
      "[0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5] 0.5\n",
      "\n",
      "mar\n",
      "\n",
      "mnar\n",
      "Missing Rate 0.5\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataname_list = [\"banknote\",\"yeast\",\"climate_model_crashes\",\n",
    "                 \"wine_quality_white\", \n",
    "                 \"yacht_hydrodynamics\",\n",
    "                 \"concrete_compression\",\n",
    "                #  \"breast_cancer\",\n",
    "                #  \"solar_fire\",\n",
    "                #  \"car_evaluation\"\n",
    "                 ]\n",
    "save = True\n",
    "p = 0.5\n",
    "\n",
    "missing_rate = [0.05,0.1,0.3, 0.5]\n",
    "#dataname_list = [\"concrete_compression\"]\n",
    "\n",
    "#dataname_list = [\"yeast\"]\n",
    "for name in dataname_list:\n",
    "    \n",
    "    print(name,p)\n",
    "    Xy = load_data(name)\n",
    "    \n",
    "    scaler = MinMaxScaler()\n",
    "    Xy['data'] = scaler.fit_transform(Xy['data'])\n",
    "    \n",
    "    if name not in [\"wine_quality_white\", \n",
    "                 \"yacht_hydrodynamics\",\n",
    "                 \"concrete_compression\"]:\n",
    "        label_encoder = LabelEncoder()\n",
    "        Xy['target'] = label_encoder.fit_transform(Xy['target'])\n",
    "    else:\n",
    "        scaler = MinMaxScaler()\n",
    "        Xy['target'] = scaler.fit_transform(Xy['target'].reshape(-1, 1))\n",
    "    feature = Xy['data']\n",
    "    label = Xy['target']\n",
    "\n",
    "    print(feature)\n",
    "    print(label)\n",
    "\n",
    "\n",
    "    #\n",
    "    #print(\"MNAR\")\n",
    "    for type in [\"mcar\",\"mar\",\"mnar\"]:\n",
    "        print(type)\n",
    "        if type == \"mcar\":\n",
    "            mask = MCAR(feature, p, seed=1)\n",
    "        elif type == \"mar\":\n",
    "            mask = MAR(feature, p)\n",
    "        elif type == \"mnar\":\n",
    "            mask = MNAR(feature, p)\n",
    "\n",
    "        for rate in missing_rate:\n",
    "            #print(rate)\n",
    "            mask_copy = mask.copy()\n",
    "            mask_copy = mask_recover(mask_copy, rate)\n",
    "\n",
    "            #calculate_missing_rates(mask_copy)\n",
    "\n",
    "            if save:\n",
    "                path = f\"data/{name}/{type}\"\n",
    "                ensure_dir(path)\n",
    "                #np.save(f\"{path}/{rate}.npy\",mask_copy)\n",
    "        print()\n",
    "        \n",
    "    \n",
    "\n",
    "    if save:\n",
    "        save_split_index_cv(Xy['data'],name,seed = 1,nfold = 5)\n",
    "        np.save(f\"data/{name}/feature.npy\", Xy['data'])\n",
    "        np.save(f\"data/{name}/label.npy\", Xy['target'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def missing_abnormal_check(mask, p):\n",
    "    n, d = mask.shape  # Get the number of rows (n) and columns (d)\n",
    "    m1 = int(p * n)  # Calculate the desired number of missing values per column\n",
    "\n",
    "    for i in range(d):  # Iterate over each column\n",
    "        m2 = np.sum(mask[:, i] == 0)  # Count the number of 0s in the column\n",
    "        if m2 == n:\n",
    "            excess = m2 - m1  # Calculate how many 0s need to be changed to 1s\n",
    "            indices = np.where(mask[:, i] == 0)[0]  # Get indices of all 0s\n",
    "            np.random.shuffle(indices)  # Randomly shuffle these indices\n",
    "            indices_to_change = indices[:excess]  # Select the first 'excess' indices\n",
    "            mask[indices_to_change, i] = 1  # Change these indices from 0 to 1\n",
    "\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Missing Mechan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MCAR(observed_values, p, seed=1):\n",
    "    np.random.seed(seed)\n",
    "    num_rows, num_cols = observed_values.shape\n",
    "    num_to_remove_per_column = int(num_rows * p)\n",
    "    masks = np.ones_like(observed_values)\n",
    "    for col in range(num_cols):\n",
    "        indices_to_remove = np.random.choice(num_rows, num_to_remove_per_column, replace=False)\n",
    "        masks[indices_to_remove, col] = 0\n",
    "    calculate_missing_rates(masks)\n",
    "    return masks\n",
    "def ensure_dir(directory):\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "def mask_recover(mask, p):\n",
    "    n = mask.shape[0]  # Number of rows in the mask\n",
    "    \n",
    "    # Calculate the missing rate for each column\n",
    "    missing_rate_per_column = np.mean(mask == 0, axis=0)\n",
    "    \n",
    "    # Calculate the necessary adjustments for each column to achieve the target missing rate p\n",
    "    adjustment_needed = np.round((missing_rate_per_column - p) * n)\n",
    "\n",
    "    # Modify the mask to approach the target missing rate p\n",
    "    for i in range(mask.shape[1]):  # Iterate over each column\n",
    "        if adjustment_needed[i] > 0:  # Too many zeros, need to convert some to ones\n",
    "            zero_indices = np.where(mask[:, i] == 0)[0]\n",
    "            np.random.seed(1)\n",
    "            np.random.shuffle(zero_indices)  # Shuffle the zero indices\n",
    "            recover_amount = int(min(adjustment_needed[i], len(zero_indices)))\n",
    "            # Set the first 'recover_amount' shuffled indices to 1\n",
    "            mask[zero_indices[:recover_amount], i] = 1\n",
    "            \n",
    "\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_missing_rates(mask):\n",
    "    # Calculate missing rate for each column\n",
    "    num_rows, num_cols = mask.shape\n",
    "    missing_rate_per_column = np.sum(mask == 0, axis=0) / num_rows\n",
    "\n",
    "    # Calculate overall missing rate\n",
    "    total_elements = num_rows * num_cols\n",
    "    overall_missing_rate = np.sum(mask == 0) / total_elements\n",
    "\n",
    "    print(np.round(missing_rate_per_column,5), np.round(overall_missing_rate,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def pick_coeffs(X, idxs_obs=None, idxs_nas=None, self_mask=False):\n",
    "    n, d = X.shape\n",
    "    if self_mask:\n",
    "        torch.manual_seed(d)\n",
    "        coeffs = torch.randn(d)\n",
    "        Wx = X * coeffs\n",
    "        coeffs /= torch.std(Wx, 0)\n",
    "    else:\n",
    "        d_obs = len(idxs_obs)\n",
    "        d_na = len(idxs_nas)\n",
    "        torch.manual_seed(d)\n",
    "        coeffs = torch.randn(d_obs, d_na).float()\n",
    "\n",
    "        # Dynamically adjust coeffs to match the type of X[:, idxs_obs]\n",
    "        if X[:, idxs_obs].dtype == torch.double:\n",
    "            coeffs = coeffs.double()\n",
    "        # Add more conditions here if there are other types you need to handle\n",
    "\n",
    "        # Perform operations\n",
    "        Wx = X[:, idxs_obs].mm(coeffs)\n",
    "        coeffs /= torch.std(Wx, 0, keepdim=True)\n",
    "    return coeffs\n",
    "\n",
    "\n",
    "def fit_intercepts(X, coeffs, p, self_mask=False):\n",
    "    if self_mask:\n",
    "        d = len(coeffs)\n",
    "        intercepts = torch.zeros(d)\n",
    "        for j in range(d):\n",
    "            def f(x):\n",
    "                return torch.sigmoid(X * coeffs[j] + x).mean().item() - p\n",
    "\n",
    "            #intercepts[j] = optimize.bisect(f, -500, 500)\n",
    "            try:\n",
    "                \n",
    "                intercepts[j] = optimize.bisect(f, -500, 500)\n",
    "            except:\n",
    "                pass\n",
    "                #print(\"Fail inters\")\n",
    "                #print(f(-500),f(500))\n",
    "    else:\n",
    "        d_obs, d_na = coeffs.shape\n",
    "        intercepts = torch.zeros(d_na)\n",
    "        for j in range(d_na):\n",
    "            def f(x):\n",
    "                return torch.sigmoid(X.mv(coeffs[:, j]) + x).mean().item() - p\n",
    "\n",
    "            #intercepts[j] = optimize.bisect(f, -500, 500)\n",
    "            \n",
    "            try:\n",
    "                intercepts[j] = optimize.bisect(f, -500, 500)\n",
    "            except:\n",
    "                pass\n",
    "                #print(\"Fail inters\")\n",
    "                #print(f(-500),f(500))       \n",
    "    return intercepts\n",
    "\n",
    "\n",
    "def MAR(X, p, p_obs = 0.5,seed = 1):\n",
    "\n",
    "    n, d = X.shape\n",
    "\n",
    "    to_torch = torch.is_tensor(X) ## output a pytorch tensor, or a numpy array\n",
    "    if not to_torch:\n",
    "        X = torch.from_numpy(X)\n",
    "\n",
    "    mask = torch.zeros(n, d).bool() if to_torch else np.zeros((n, d)).astype(bool)\n",
    "\n",
    "    d_obs = max(int(p_obs * d), 1) ## number of variables that will have no missing values (at least one variable)\n",
    "    d_na = d - d_obs ## number of variables that will have missing values\n",
    "\n",
    "    ### Sample variables that will all be observed, and those with missing values:\n",
    "    np.random.seed(n)\n",
    "    idxs_obs = np.random.choice(d, d_obs, replace=False)\n",
    "    idxs_nas = np.array([i for i in range(d) if i not in idxs_obs])\n",
    "\n",
    "    ### Other variables will have NA proportions that depend on those observed variables, through a logistic model\n",
    "    ### The parameters of this logistic model are random.\n",
    "\n",
    "    ### Pick coefficients so that W^Tx has unit variance (avoids shrinking)\n",
    "    coeffs = pick_coeffs(X, idxs_obs, idxs_nas)\n",
    "    ### Pick the intercepts to have a desired amount of missing values\n",
    "    intercepts = fit_intercepts(X[:, idxs_obs], coeffs, p)\n",
    "\n",
    "    ps = torch.sigmoid(X[:, idxs_obs].mm(coeffs) + intercepts)\n",
    "    torch.manual_seed(n)\n",
    "    ber = torch.rand(n, d_na)\n",
    "    mask[:, idxs_nas] = 1- (ber < ps).int()\n",
    "    mask = missing_abnormal_check(mask, p)\n",
    "    #calculate_missing_rates(mask)\n",
    "\n",
    "    return mask\n",
    "\n",
    "#\n",
    "\n",
    "def MNAR(X, p):\n",
    "    print(\"Missing Rate\",p)\n",
    "\n",
    "    n, d = X.shape\n",
    "\n",
    "    to_torch = torch.is_tensor(X) ## output a pytorch tensor, or a numpy array\n",
    "    if not to_torch:\n",
    "        X = torch.from_numpy(X)\n",
    "\n",
    "    ### Variables will have NA proportions that depend on those observed variables, through a logistic model\n",
    "    ### The parameters of this logistic model are random.\n",
    "\n",
    "    ### Pick coefficients so that W^Tx has unit variance (avoids shrinking)\n",
    "    coeffs = pick_coeffs(X, self_mask=True)\n",
    "    \n",
    "    ### Pick the intercepts to have a desired amount of missing values\n",
    "    intercepts = fit_intercepts(X, coeffs, p, self_mask=True)\n",
    "    \n",
    "\n",
    "    ps = torch.sigmoid(X * coeffs + intercepts)\n",
    "\n",
    "\n",
    "    np.random.seed(n)\n",
    "    ber = np.random.rand(n, d)\n",
    "    mask = 1-(ber < ps if to_torch else ber < ps.numpy()).astype(int)\n",
    "    mask = missing_abnormal_check(mask, p)\n",
    "    #calculate_missing_rates(mask)\n",
    "    \n",
    "    #mask = 1-mask\n",
    "    return mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
