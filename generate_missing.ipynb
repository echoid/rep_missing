{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "#from ucimlrepo import fetch_ucirepo \n",
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "#import torch\n",
    "from scipy import optimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(suppress=True, precision=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataname_list = [\"banknote\",\"yeast\",\"climate_model_crashes\",\n",
    "                 \"wine_quality_white\", \"yacht_hydrodynamics\",\"concrete_compression\",\n",
    "                 \"breast_cancer\",\"solar_fire\",\"car_evluation\"\n",
    "                 ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_split_index_cv(scaled_data, directory_path, seed=1, nfold=5):\n",
    "    indlist = np.arange(len(scaled_data))\n",
    "\n",
    "    np.random.seed(seed)\n",
    "    np.random.shuffle(indlist)\n",
    "\n",
    "    fold_size = len(scaled_data) // nfold\n",
    "    save_index = {}\n",
    "\n",
    "    for fold in range(nfold):\n",
    "        start = fold * fold_size\n",
    "        end = start + fold_size if fold < nfold - 1 else len(scaled_data)\n",
    "        \n",
    "        test_index = indlist[start:end]\n",
    "        train_index = np.concatenate([indlist[:start], indlist[end:]])\n",
    "\n",
    "        # If you want to split the training set into train and validation sets\n",
    "        num_train = int(len(train_index) * 0.9)\n",
    "        train_subindex = train_index[:num_train]\n",
    "        valid_subindex = train_index[num_train:]\n",
    "\n",
    "        fold_index = {\n",
    "            \"test_index\": test_index.astype(np.int64).tolist(),\n",
    "            \"train_index\": train_subindex.astype(np.int64).tolist(),\n",
    "            \"valid_index\": valid_subindex.astype(np.int64).tolist()\n",
    "        }\n",
    "        save_index[f\"fold_{fold+1}\"] = fold_index\n",
    "\n",
    "    with open(f\"data/{directory_path}/split_index_cv_seed-{seed}_nfold-{nfold}.json\", 'w') as file:\n",
    "        json.dump(save_index, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(name):\n",
    "    if name == \"banknote\":\n",
    "        with open('data/banknote/data_banknote_authentication.txt', 'rb') as f:\n",
    "            df = pd.read_csv(f, low_memory=False, sep=',',header = None)\n",
    "            Xy = {}\n",
    "            # Ignore the two blocking factor\n",
    "            Xy['data'] = df.values[:, :-1]\n",
    "            Xy['target'] =  df.values[:, -1]\n",
    "    elif name == \"yeast\":\n",
    "        with open('data/yeast/yeast.data', 'rb') as f:\n",
    "            df = pd.read_csv(f, delimiter='\\s+', header = None)\n",
    "            Xy = {}\n",
    "            # remove index\n",
    "            Xy['data'] = df.values[:, 1:-1].astype('float')\n",
    "            Xy['target'] =  df.values[:, -1]\n",
    "    elif name == \"climate_model_crashes\":\n",
    "        with open('data/climate_model_crashes/pop_failures.dat', 'rb') as f:\n",
    "            df = pd.read_csv(f, delimiter='\\s+', header = 0)\n",
    "            Xy = {}\n",
    "            # Ignore the two blocking factor\n",
    "            Xy['data'] = df.values[:, 2:-1]\n",
    "            Xy['target'] =  df.values[:, -1]\n",
    "    elif name == \"wine_quality_white\":\n",
    "        with open('data/wine_quality_white/data.csv', 'rb') as f:\n",
    "            df = pd.read_csv(f, delimiter=';')\n",
    "            Xy = {}\n",
    "            Xy['data'] = df.values[:, :-1].astype('float')\n",
    "            Xy['target'] =  df.values[:, -1]\n",
    "\n",
    "    elif name == \"yacht_hydrodynamics\":\n",
    "        with open('data/yacht_hydrodynamics/yacht_hydrodynamics.data', 'rb') as f:\n",
    "            df = pd.read_csv(f, delimiter='\\s+', header = None)\n",
    "            Xy = {}\n",
    "            Xy['data'] = df.values[:, :-1]\n",
    "            Xy['target'] =  df.values[:, -1]\n",
    "\n",
    "    elif name == \"concrete_compression\":\n",
    "        with open('data/concrete_compression/Concrete_Data.xls', 'rb') as f:\n",
    "            df = pd.read_excel(io=f)\n",
    "            Xy = {}\n",
    "            Xy['data'] = df.values[:, :-1]\n",
    "            Xy['target'] =  df.values[:, -1]\n",
    "\n",
    "    elif name == \"breast_cancer\":\n",
    "        with open('data/breast_cancer/breast_cancer.data', 'rb') as f:\n",
    "            df = pd.read_csv(f, delimiter=',', header = None)\n",
    "            Xy = {}\n",
    "            Xy['data'] = df.values[:, :-1]\n",
    "            Xy['target'] =  df.values[:, -1]\n",
    "\n",
    "    elif name == \"solar_fire\":\n",
    "        with open('data/solar_fire/flare.data1', 'rb') as f:\n",
    "            df1 = pd.read_csv(f, delimiter='\\s+', header = None)\n",
    "        with open('data/solar_fire/flare.data2', 'rb') as f:\n",
    "            df2 = pd.read_csv(f, delimiter='\\s+', header = None)\n",
    "            df = pd.concat([df1, df2], ignore_index=True)\n",
    "            Xy = {}\n",
    "            Xy['data'] = df.values[:, :-1]\n",
    "            Xy['target'] =  df.values[:, -1]\n",
    "\n",
    "    elif name == \"car_evaluation\":\n",
    "        with open('data/car_evaluation/car.data', 'rb') as f:\n",
    "            df = pd.read_csv(f, delimiter=',', header = None)\n",
    "            Xy = {}\n",
    "            Xy['data'] = df.values[:, :-1]\n",
    "            Xy['target'] =  df.values[:, -1]\n",
    "\n",
    "    return Xy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "banknote 0.5\n",
      "mcar\n",
      "[0.5 0.5 0.5 0.5] 0.5\n",
      "\n",
      "mar\n",
      "\n",
      "mnar\n",
      "Missing Rate 0.5\n",
      "\n",
      "yeast 0.5\n",
      "mcar\n",
      "[0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5] 0.5\n",
      "\n",
      "mar\n",
      "\n",
      "mnar\n",
      "Missing Rate 0.5\n",
      "\n",
      "climate_model_crashes 0.5\n",
      "mcar\n",
      "[0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5] 0.5\n",
      "\n",
      "mar\n",
      "\n",
      "mnar\n",
      "Missing Rate 0.5\n",
      "\n",
      "wine_quality_white 0.5\n",
      "mcar\n",
      "[0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5] 0.5\n",
      "\n",
      "mar\n",
      "\n",
      "mnar\n",
      "Missing Rate 0.5\n",
      "\n",
      "yacht_hydrodynamics 0.5\n",
      "mcar\n",
      "[0.5 0.5 0.5 0.5 0.5 0.5] 0.5\n",
      "\n",
      "mar\n",
      "\n",
      "mnar\n",
      "Missing Rate 0.5\n",
      "\n",
      "concrete_compression 0.5\n",
      "mcar\n",
      "[0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5] 0.5\n",
      "\n",
      "mar\n",
      "\n",
      "mnar\n",
      "Missing Rate 0.5\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataname_list = [\"banknote\",\"yeast\",\"climate_model_crashes\",\n",
    "                 \"wine_quality_white\", \n",
    "                 \"yacht_hydrodynamics\",\n",
    "                 \"concrete_compression\",\n",
    "                #  \"breast_cancer\",\n",
    "                #  \"solar_fire\",\n",
    "                #  \"car_evaluation\"\n",
    "                 ]\n",
    "save = True\n",
    "p = 0.5\n",
    "\n",
    "missing_rate = [0.05,0.1,0.3, 0.5]\n",
    "\n",
    "\n",
    "#dataname_list = [\"yeast\"]\n",
    "for name in dataname_list:\n",
    "    \n",
    "    print(name,p)\n",
    "    Xy = load_data(name)\n",
    "    \n",
    "    feature = Xy['data']\n",
    "    label = Xy['target']\n",
    "\n",
    "\n",
    "    #\n",
    "    #print(\"MNAR\")\n",
    "    for type in [\"mcar\",\"mar\",\"mnar\"]:\n",
    "        print(type)\n",
    "        if type == \"mcar\":\n",
    "            mask = MCAR(feature, p, seed=1)\n",
    "        elif type == \"mar\":\n",
    "            mask = MAR(feature, p)\n",
    "        elif type == \"mnar\":\n",
    "            mask = MNAR(feature, p)\n",
    "\n",
    "        for rate in missing_rate:\n",
    "            #print(rate)\n",
    "            mask_copy = mask.copy()\n",
    "            mask_copy = mask_recover(mask_copy, rate)\n",
    "\n",
    "            #calculate_missing_rates(mask_copy)\n",
    "\n",
    "            if save:\n",
    "                path = f\"data/{name}/{type}\"\n",
    "                ensure_dir(path)\n",
    "                np.save(f\"{path}/{rate}.npy\",mask_copy)\n",
    "        print()\n",
    "        \n",
    "    \n",
    "\n",
    "    if save:\n",
    "        save_split_index_cv(Xy['data'],name,seed = 1,nfold = 5)\n",
    "        np.save(f\"data/{name}/feature.npy\", Xy['data'])\n",
    "        np.save(f\"data/{name}/label.npy\", Xy['target'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def missing_abnormal_check(mask, p):\n",
    "    n, d = mask.shape  # Get the number of rows (n) and columns (d)\n",
    "    m1 = int(p * n)  # Calculate the desired number of missing values per column\n",
    "\n",
    "    for i in range(d):  # Iterate over each column\n",
    "        m2 = np.sum(mask[:, i] == 0)  # Count the number of 0s in the column\n",
    "        if m2 == n:\n",
    "            excess = m2 - m1  # Calculate how many 0s need to be changed to 1s\n",
    "            indices = np.where(mask[:, i] == 0)[0]  # Get indices of all 0s\n",
    "            np.random.shuffle(indices)  # Randomly shuffle these indices\n",
    "            indices_to_change = indices[:excess]  # Select the first 'excess' indices\n",
    "            mask[indices_to_change, i] = 1  # Change these indices from 0 to 1\n",
    "\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Missing Mechan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MCAR(observed_values, p, seed=1):\n",
    "    np.random.seed(seed)\n",
    "    num_rows, num_cols = observed_values.shape\n",
    "    num_to_remove_per_column = int(num_rows * p)\n",
    "    masks = np.ones_like(observed_values)\n",
    "    for col in range(num_cols):\n",
    "        indices_to_remove = np.random.choice(num_rows, num_to_remove_per_column, replace=False)\n",
    "        masks[indices_to_remove, col] = 0\n",
    "    calculate_missing_rates(masks)\n",
    "    return masks\n",
    "def ensure_dir(directory):\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "def mask_recover(mask, p):\n",
    "    n = mask.shape[0]  # Number of rows in the mask\n",
    "    \n",
    "    # Calculate the missing rate for each column\n",
    "    missing_rate_per_column = np.mean(mask == 0, axis=0)\n",
    "    \n",
    "    # Calculate the necessary adjustments for each column to achieve the target missing rate p\n",
    "    adjustment_needed = np.round((missing_rate_per_column - p) * n)\n",
    "\n",
    "    # Modify the mask to approach the target missing rate p\n",
    "    for i in range(mask.shape[1]):  # Iterate over each column\n",
    "        if adjustment_needed[i] > 0:  # Too many zeros, need to convert some to ones\n",
    "            zero_indices = np.where(mask[:, i] == 0)[0]\n",
    "            np.random.seed(1)\n",
    "            np.random.shuffle(zero_indices)  # Shuffle the zero indices\n",
    "            recover_amount = int(min(adjustment_needed[i], len(zero_indices)))\n",
    "            # Set the first 'recover_amount' shuffled indices to 1\n",
    "            mask[zero_indices[:recover_amount], i] = 1\n",
    "            \n",
    "\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_missing_rates(mask):\n",
    "    # Calculate missing rate for each column\n",
    "    num_rows, num_cols = mask.shape\n",
    "    missing_rate_per_column = np.sum(mask == 0, axis=0) / num_rows\n",
    "\n",
    "    # Calculate overall missing rate\n",
    "    total_elements = num_rows * num_cols\n",
    "    overall_missing_rate = np.sum(mask == 0) / total_elements\n",
    "\n",
    "    print(np.round(missing_rate_per_column,5), np.round(overall_missing_rate,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def pick_coeffs(X, idxs_obs=None, idxs_nas=None, self_mask=False):\n",
    "    n, d = X.shape\n",
    "    if self_mask:\n",
    "        torch.manual_seed(d)\n",
    "        coeffs = torch.randn(d)\n",
    "        Wx = X * coeffs\n",
    "        coeffs /= torch.std(Wx, 0)\n",
    "    else:\n",
    "        d_obs = len(idxs_obs)\n",
    "        d_na = len(idxs_nas)\n",
    "        torch.manual_seed(d)\n",
    "        coeffs = torch.randn(d_obs, d_na).float()\n",
    "\n",
    "        # Dynamically adjust coeffs to match the type of X[:, idxs_obs]\n",
    "        if X[:, idxs_obs].dtype == torch.double:\n",
    "            coeffs = coeffs.double()\n",
    "        # Add more conditions here if there are other types you need to handle\n",
    "\n",
    "        # Perform operations\n",
    "        Wx = X[:, idxs_obs].mm(coeffs)\n",
    "        coeffs /= torch.std(Wx, 0, keepdim=True)\n",
    "    return coeffs\n",
    "\n",
    "\n",
    "def fit_intercepts(X, coeffs, p, self_mask=False):\n",
    "    if self_mask:\n",
    "        d = len(coeffs)\n",
    "        intercepts = torch.zeros(d)\n",
    "        for j in range(d):\n",
    "            def f(x):\n",
    "                return torch.sigmoid(X * coeffs[j] + x).mean().item() - p\n",
    "\n",
    "            #intercepts[j] = optimize.bisect(f, -500, 500)\n",
    "            try:\n",
    "                \n",
    "                intercepts[j] = optimize.bisect(f, -500, 500)\n",
    "            except:\n",
    "                pass\n",
    "                #print(\"Fail inters\")\n",
    "                #print(f(-500),f(500))\n",
    "    else:\n",
    "        d_obs, d_na = coeffs.shape\n",
    "        intercepts = torch.zeros(d_na)\n",
    "        for j in range(d_na):\n",
    "            def f(x):\n",
    "                return torch.sigmoid(X.mv(coeffs[:, j]) + x).mean().item() - p\n",
    "\n",
    "            #intercepts[j] = optimize.bisect(f, -500, 500)\n",
    "            \n",
    "            try:\n",
    "                intercepts[j] = optimize.bisect(f, -500, 500)\n",
    "            except:\n",
    "                pass\n",
    "                #print(\"Fail inters\")\n",
    "                #print(f(-500),f(500))       \n",
    "    return intercepts\n",
    "\n",
    "\n",
    "def MAR(X, p, p_obs = 0.5,seed = 1):\n",
    "\n",
    "    n, d = X.shape\n",
    "\n",
    "    to_torch = torch.is_tensor(X) ## output a pytorch tensor, or a numpy array\n",
    "    if not to_torch:\n",
    "        X = torch.from_numpy(X)\n",
    "\n",
    "    mask = torch.zeros(n, d).bool() if to_torch else np.zeros((n, d)).astype(bool)\n",
    "\n",
    "    d_obs = max(int(p_obs * d), 1) ## number of variables that will have no missing values (at least one variable)\n",
    "    d_na = d - d_obs ## number of variables that will have missing values\n",
    "\n",
    "    ### Sample variables that will all be observed, and those with missing values:\n",
    "    np.random.seed(n)\n",
    "    idxs_obs = np.random.choice(d, d_obs, replace=False)\n",
    "    idxs_nas = np.array([i for i in range(d) if i not in idxs_obs])\n",
    "\n",
    "    ### Other variables will have NA proportions that depend on those observed variables, through a logistic model\n",
    "    ### The parameters of this logistic model are random.\n",
    "\n",
    "    ### Pick coefficients so that W^Tx has unit variance (avoids shrinking)\n",
    "    coeffs = pick_coeffs(X, idxs_obs, idxs_nas)\n",
    "    ### Pick the intercepts to have a desired amount of missing values\n",
    "    intercepts = fit_intercepts(X[:, idxs_obs], coeffs, p)\n",
    "\n",
    "    ps = torch.sigmoid(X[:, idxs_obs].mm(coeffs) + intercepts)\n",
    "    torch.manual_seed(n)\n",
    "    ber = torch.rand(n, d_na)\n",
    "    mask[:, idxs_nas] = 1- (ber < ps).int()\n",
    "    mask = missing_abnormal_check(mask, p)\n",
    "    #calculate_missing_rates(mask)\n",
    "\n",
    "    return mask\n",
    "\n",
    "#\n",
    "\n",
    "def MNAR(X, p):\n",
    "    print(\"Missing Rate\",p)\n",
    "\n",
    "    n, d = X.shape\n",
    "\n",
    "    to_torch = torch.is_tensor(X) ## output a pytorch tensor, or a numpy array\n",
    "    if not to_torch:\n",
    "        X = torch.from_numpy(X)\n",
    "\n",
    "    ### Variables will have NA proportions that depend on those observed variables, through a logistic model\n",
    "    ### The parameters of this logistic model are random.\n",
    "\n",
    "    ### Pick coefficients so that W^Tx has unit variance (avoids shrinking)\n",
    "    coeffs = pick_coeffs(X, self_mask=True)\n",
    "    \n",
    "    ### Pick the intercepts to have a desired amount of missing values\n",
    "    intercepts = fit_intercepts(X, coeffs, p, self_mask=True)\n",
    "    \n",
    "\n",
    "    ps = torch.sigmoid(X * coeffs + intercepts)\n",
    "\n",
    "\n",
    "    np.random.seed(n)\n",
    "    ber = np.random.rand(n, d)\n",
    "    mask = 1-(ber < ps if to_torch else ber < ps.numpy()).astype(int)\n",
    "    mask = missing_abnormal_check(mask, p)\n",
    "    #calculate_missing_rates(mask)\n",
    "    \n",
    "    #mask = 1-mask\n",
    "    return mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
