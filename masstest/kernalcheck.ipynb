{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import tomllib\n",
    "import numpy as np\n",
    "from sklearn import svm\n",
    "import pandas as pd \n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import KernelPCA\n",
    "\n",
    "import masskernal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_value = None\n",
    "nbins = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(numerical = True):\n",
    "    if numerical:\n",
    "        X = np.load(\"BNfeature.npy\")\n",
    "        y = np.load(\"BNlabel.npy\")\n",
    "        data_stats = None\n",
    "    else:\n",
    "        data = pd.read_csv(\"chess_shuffled_test.csv\", header = None)\n",
    "        with open(\"chess_shuffled_test.csv.toml\", mode = \"rb\") as f:\n",
    "            data_stats = tomllib.load(f)\n",
    "        # Split the DataFrame into X and Y\n",
    "        X = data.iloc[:, :-1].to_numpy().astype(np.float64)  # Convert DataFrame to NumPy array for features\n",
    "        y = data.iloc[:, -1].to_numpy().astype(np.float64)   # Convert DataFrame to NumPy array for target\n",
    "\n",
    "    return X, y, data_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y, data_stats = load_data(numerical = True)\n",
    "\n",
    "train_X, test_X, train_Y, test_Y = train_test_split(X, Y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## After Applied Missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = make_missing(train_X,missing_rate = 0.5)\n",
    "test_X = make_missing(test_X,missing_rate = 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 5. 2. 0.]\n",
      " [0. 4. 1. 0.]\n",
      " [0. 0. 4. 1.]\n",
      " ...\n",
      " [0. 4. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 4. 2.]]\n",
      "Original Kernal | MCAR 0.2 | F1 Score: 0.7345\n",
      "Original Kernal | MCAR 0.2 | Accuracy: 0.7345\n"
     ]
    }
   ],
   "source": [
    "m0_krn_ori = masskernal.M0_Kernel(nbins, data_stats)\n",
    "m0_krn_ori.set_nbins(param_value)\n",
    "train, test = m0_krn_ori.build_model(train_X, test_X)  # this does the pre-processing step\n",
    "sim_train = m0_krn_ori.transform(train)\n",
    "sim_test = m0_krn_ori.transform(test,train)  # row = train, col = test\n",
    "\n",
    "print(train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.40770468, 0.98319906, 0.25749155,        nan],\n",
       "       [       nan, 0.77197125, 0.18174769,        nan],\n",
       "       [       nan, 0.22113625, 0.73770866, 0.60470826],\n",
       "       ...,\n",
       "       [       nan, 0.86310417,        nan,        nan],\n",
       "       [       nan,        nan,        nan,        nan],\n",
       "       [       nan,        nan, 0.72061516, 0.72533984]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_X"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use modified Kernal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 3. 10.  5. 10.]\n",
      " [10.  8.  3.  0.]\n",
      " [10.  0. 10.  3.]\n",
      " ...\n",
      " [10.  9. 10.  0.]\n",
      " [10.  0. 10.  0.]\n",
      " [10.  0. 10.  5.]]\n"
     ]
    }
   ],
   "source": [
    "mo_krn = Mo_Kernel(nbins, data_stats)\n",
    "mo_krn.set_nbins(param_value)\n",
    "train, test = mo_krn.build_model(train_X, test_X)  # this does the pre-processing step\n",
    "sim_train = mo_krn.transform(train)\n",
    "sim_test = mo_krn.transform(test,train)  # row = train, col = test\n",
    "\n",
    "print(train)\n",
    "\n",
    "# # Configure Kernel PCA for precomputed kernels\n",
    "# kpca = KernelPCA(kernel='precomputed')\n",
    "# X_kpca_train = kpca.fit_transform(sim_train)\n",
    "# X_kpca_test = kpca.transform(sim_test)\n",
    "\n",
    "\n",
    "# rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "# rf.fit(X_kpca_train, train_Y)\n",
    "# y_pred = rf.predict(X_kpca_test)\n",
    "# f1 = f1_score(test_Y, y_pred,average=\"macro\")\n",
    "# acc = accuracy_score(test_Y, y_pred)\n",
    "\n",
    "\n",
    "# print(f\"After Modified Kernal | MCAR 0.2 | F1 Score: {f1:.4f}\")\n",
    "# print(f\"After Modified Kernal | MCAR 0.2 | Accuracy: {acc:.4f}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do not change transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m0_krn = M0_Kernel(nbins, data_stats)\n",
    "m0_krn.set_nbins(param_value)\n",
    "train, test = m0_krn.build_model(train_X, test_X)  # this does the pre-processing step\n",
    "print(\"- Sim: Train\")\n",
    "sim_train = m0_krn.transform_keep(train)\n",
    "\n",
    "print(\"- Sim: Train/Test\")\n",
    "sim_test = m0_krn.transform_keep(test,train)  # row = train, col = test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure Kernel PCA for precomputed kernels\n",
    "kpca = KernelPCA(kernel='precomputed')\n",
    "\n",
    "# Transform data using Kernel PCA\n",
    "X_kpca_train = kpca.fit_transform(sim_train)\n",
    "X_kpca_test = kpca.transform(sim_test)\n",
    "\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# Train the classifier using the Kernel PCA transformed data\n",
    "rf.fit(X_kpca_train, train_Y)\n",
    "# Predict using the precomputed test kernel matrix\n",
    "y_pred = rf.predict(X_kpca_test)\n",
    "f1 = f1_score(test_Y, y_pred,average=\"macro\")\n",
    "acc = accuracy_score(test_Y, y_pred)\n",
    "\n",
    "\n",
    "print(f\"After No-transform Kernal | MCAR 0.2 | F1 Score: {f1:.4f}\")\n",
    "print(f\"After No-transform Kernal | MCAR 0.2 | Accuracy: {acc:.4f}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mix-Type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y, data_stats = load_data(numerical = False)\n",
    "\n",
    "train_X, test_X, train_Y, test_Y = train_test_split(X, Y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Complete Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before Kernal | Complete Data | F1 Score: 0.9874\n",
      "Before Kernal | Complete Data |  Accuracy: 0.9875\n"
     ]
    }
   ],
   "source": [
    "# Calculate F1 score and accuracy\n",
    "clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "clf.fit(train_X, train_Y)\n",
    "# Predict using the precomputed test kernel matrix\n",
    "y_pred = clf.predict(test_X)\n",
    "\n",
    "\n",
    "f1 = f1_score(test_Y, y_pred,average=\"macro\")\n",
    "acc = accuracy_score(test_Y, y_pred)\n",
    "print(f\"Before Kernal | Complete Data | F1 Score: {f1:.4f}\")\n",
    "print(f\"Before Kernal | Complete Data |  Accuracy: {acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Sim: Train\n",
      "- Sim: Train/Test\n"
     ]
    }
   ],
   "source": [
    "m0_krn = M0_Kernel(nbins, data_stats)\n",
    "m0_krn.set_nbins(param_value)\n",
    "train, test = m0_krn.build_model(train_X, test_X)  # this does the pre-processing step\n",
    "\n",
    "\n",
    "print(\"- Sim: Train\")\n",
    "sim_train = m0_krn.transform(train)\n",
    "\n",
    "print(\"- Sim: Train/Test\")\n",
    "sim_test = m0_krn.transform(test,train)  # row = train, col = test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After Kernal | Complete Data | F1 Score: 0.9529\n",
      "After Kernal | Complete Data |  Accuracy: 0.9531\n"
     ]
    }
   ],
   "source": [
    "# Configure Kernel PCA for precomputed kernels\n",
    "kpca = KernelPCA(kernel='precomputed')\n",
    "\n",
    "# Transform data using Kernel PCA\n",
    "X_kpca_train = kpca.fit_transform(sim_train)\n",
    "X_kpca_test = kpca.transform(sim_test)\n",
    "\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# Train the classifier using the Kernel PCA transformed data\n",
    "rf.fit(X_kpca_train, train_Y)\n",
    "# Predict using the precomputed test kernel matrix\n",
    "y_pred = rf.predict(X_kpca_test)\n",
    "f1 = f1_score(test_Y, y_pred,average=\"macro\")\n",
    "acc = accuracy_score(test_Y, y_pred)\n",
    "\n",
    "\n",
    "print(f\"After Kernal | Complete Data | F1 Score: {f1:.4f}\")\n",
    "print(f\"After Kernal | Complete Data |  Accuracy: {acc:.4f}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## After Applied Missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y, data_stats = load_data(numerical = False)\n",
    "\n",
    "train_X, test_X, train_Y, test_Y = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
    "\n",
    "train_X = make_missing(train_X,missing_rate = 0.2)\n",
    "test_X = make_missing(test_X,missing_rate = 0.2)\n",
    "\n",
    "\n",
    "# Assumpe just fill in a new group\n",
    "train_X[np.isnan(train_X)] = 1\n",
    "test_X[np.isnan(test_X)] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Sim: Train\n",
      "- Sim: Train/Test\n"
     ]
    }
   ],
   "source": [
    "m0_krn_ori = masskernal.M0_Kernel(nbins, data_stats)\n",
    "m0_krn_ori.set_nbins(param_value)\n",
    "train, test = m0_krn_ori.build_model(train_X, test_X)  # this does the pre-processing step\n",
    "print(\"- Sim: Train\")\n",
    "sim_train = m0_krn_ori.transform(train)\n",
    "\n",
    "print(\"- Sim: Train/Test\")\n",
    "sim_test = m0_krn_ori.transform(test,train)  # row = train, col = test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After Original Kernal | MCAR 0.2 | F1 Score: 0.7808\n",
      "After Original Kernal | MCAR 0.2 | Accuracy: 0.7828\n"
     ]
    }
   ],
   "source": [
    "# Configure Kernel PCA for precomputed kernels\n",
    "kpca = KernelPCA(kernel='precomputed')\n",
    "\n",
    "# Transform data using Kernel PCA\n",
    "X_kpca_train = kpca.fit_transform(sim_train)\n",
    "X_kpca_test = kpca.transform(sim_test)\n",
    "\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# Train the classifier using the Kernel PCA transformed data\n",
    "rf.fit(X_kpca_train, train_Y)\n",
    "# Predict using the precomputed test kernel matrix\n",
    "y_pred = rf.predict(X_kpca_test)\n",
    "f1 = f1_score(test_Y, y_pred,average=\"macro\")\n",
    "acc = accuracy_score(test_Y, y_pred)\n",
    "\n",
    "# using -1 to impute missing data\n",
    "print(f\"After Original Kernal | MCAR 0.2 | F1 Score: {f1:.4f}\")\n",
    "print(f\"After Original Kernal | MCAR 0.2 | Accuracy: {acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use modified Kernel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y, data_stats = load_data(numerical = False)\n",
    "\n",
    "train_X, test_X, train_Y, test_Y = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
    "\n",
    "train_X = make_missing(train_X,missing_rate = 0.2)\n",
    "test_X = make_missing(test_X,missing_rate = 0.2)\n",
    "\n",
    "# Assumpe just fill in a new group\n",
    "train_X[np.isnan(train_X)] = -1\n",
    "test_X[np.isnan(test_X)] = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "m0_krn = M0_Kernel(nbins, data_stats)\n",
    "m0_krn.set_nbins(param_value)\n",
    "train, test = m0_krn.build_model(train_X, test_X)  # this does the pre-processing step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Sim: Train\n",
      "- Sim: Train/Test\n"
     ]
    }
   ],
   "source": [
    "print(\"- Sim: Train\")\n",
    "sim_train = m0_krn.transform_ordinal(train)\n",
    "\n",
    "print(\"- Sim: Train/Test\")\n",
    "sim_test = m0_krn.transform_ordinal(test,train)  # row = train, col = test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After Modified Kernal | MCAR 0.2 | F1 Score: 0.8062\n",
      "After Modified Kernal | MCAR 0.2 | Accuracy: 0.8078\n"
     ]
    }
   ],
   "source": [
    "# Configure Kernel PCA for precomputed kernels\n",
    "kpca = KernelPCA(kernel='precomputed')\n",
    "\n",
    "# Transform data using Kernel PCA\n",
    "X_kpca_train = kpca.fit_transform(sim_train)\n",
    "X_kpca_test = kpca.transform(sim_test)\n",
    "\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# Train the classifier using the Kernel PCA transformed data\n",
    "rf.fit(X_kpca_train, train_Y)\n",
    "# Predict using the precomputed test kernel matrix\n",
    "y_pred = rf.predict(X_kpca_test)\n",
    "f1 = f1_score(test_Y, y_pred,average=\"macro\")\n",
    "acc = accuracy_score(test_Y, y_pred)\n",
    "\n",
    "print(f\"After Modified Kernal | MCAR 0.2 | F1 Score: {f1:.4f}\")\n",
    "print(f\"After Modified Kernal | MCAR 0.2 | Accuracy: {acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_distance(num, compare_to):\n",
    "    # Calculate absolute differences\n",
    "    distance_to_zero = abs(num - 0)\n",
    "    distance_to_compare = abs(num - compare_to)\n",
    "\n",
    "    # Determine which is closer\n",
    "    if distance_to_zero > distance_to_compare:\n",
    "        return 0\n",
    "    elif distance_to_zero < distance_to_compare:\n",
    "        return compare_to-1\n",
    "    else:\n",
    "        return num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ctypes import c_float\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "class Mo_Kernel:\n",
    "    def __init__(self, nbins = None, stats = None):\n",
    "        self.nbins_ = nbins\n",
    "        self.stats_ = stats\n",
    "\n",
    "    def build_model(self, train, test):\n",
    "\n",
    "        # if data missing, -1 will be inputed\n",
    "        def get_bin_dissimilarity():\n",
    "            bin_dissim = [[] for i in range(self.ndim_)]\n",
    "            max_num_bins = max(self.num_bins_)\n",
    "\n",
    "            for i in range(self.ndim_):\n",
    "                n_bins = self.num_bins_[i]\n",
    "                bin_cf = [0 for j in range(n_bins)]\n",
    "                cf = 0\n",
    "\n",
    "                if (self.stats_ is not None) and (\"Nominal\" in self.stats_[\"attribute\"][i][\"type\"]):\n",
    "                    for j in range(n_bins):\n",
    "                        bin_cf[j] = self.bin_counts_[i][j]\n",
    "                else:\n",
    "                    for j in range(n_bins):\n",
    "                        cf = cf + self.bin_counts_[i][j]\n",
    "                        bin_cf[j] = cf\n",
    "\n",
    "                b_mass = [[0.0 for j in range(max_num_bins)] for k in range(max_num_bins)]\n",
    "\n",
    "                for j in range(n_bins):\n",
    "                    for k in range(j, n_bins):\n",
    "                        if (self.stats_ is not None) and (\"Nominal\" in self.stats_[\"attribute\"][i][\"type\"]):\n",
    "                            if j == k:\n",
    "                                prob_mass = (bin_cf[k] + 1) / (self.ndata_ + n_bins)\n",
    "                            else:\n",
    "                                prob_mass = (bin_cf[k] + bin_cf[j] + 1) / (self.ndata_ + n_bins)\n",
    "                        else:\n",
    "                            prob_mass = (bin_cf[k] - bin_cf[j] + self.bin_counts_[i][j] + 1) / (self.ndata_ + n_bins)\n",
    "\n",
    "                        b_mass[j][k] = np.log(prob_mass)\n",
    "                        b_mass[k][j] = b_mass[j][k]\n",
    "\n",
    "                bin_dissim[i] = b_mass\n",
    "            return np.array(bin_dissim)\n",
    "\n",
    "        self.ndata_ = len(train) # number of train instance\n",
    "        self.ndim_ = len(train[0]) # number of train column\n",
    "\n",
    "        if self.nbins_ is None: # pre-define bin numbers\n",
    "            self.nbins_ = int(np.log2(self.ndata_) + 1)\n",
    "\n",
    "\n",
    "        self.dimVec_ = np.array([i for i in range(self.ndim_)])\n",
    "        self.discretiser_ = EqualFrequencyDiscretizer(train, self.nbins_, self.stats_)\n",
    "        self.bin_cuts_, self.bin_counts_ = self.discretiser_.get_bin_cuts_counts()\n",
    "        self.num_bins_ = self.discretiser_.get_num_bins()\n",
    "        self.bin_dissimilarities_ = get_bin_dissimilarity()\n",
    "\n",
    "        new_test = []\n",
    "\n",
    "        for i in range(len(test)):\n",
    "            # make each column into bin_id\n",
    "            new_test.append(self.discretiser_.get_bin_id(test[i, :]))\n",
    "        \n",
    "        \n",
    "        return self.discretiser_.get_data_bin_id(), np.array(new_test, dtype = c_float, order = \"C\")\n",
    "\n",
    "    def set_nbins(self, nbins):\n",
    "        self.nbins_ = nbins\n",
    "\n",
    "    def transform(self, train, test=None):\n",
    "        def convert(x_bin_ids, y_bin_ids):\n",
    "            if -1 in x_bin_ids or -1 in y_bin_ids:\n",
    "                for i, bin_id in enumerate(x_bin_ids):\n",
    "                    if bin_id == -1:\n",
    "                        #print(\"Before\",i,bin_id,y_bin_ids[i])\n",
    "                        x_bin_ids[i] = max_distance(bin_id, self.nbins_)\n",
    "                        #print(\"After\",i,x_bin_ids[i],y_bin_ids[i])\n",
    "                    elif y_bin_ids[i] == -1:\n",
    "                        #print(\"Before\",i,bin_id,y_bin_ids[i])\n",
    "                        y_bin_ids[i] = max_distance(bin_id, self.nbins_)\n",
    "                        #print(\"After\",i,x_bin_ids[i],y_bin_ids[i])\n",
    "                    elif (bin_id == -1) and (y_bin_ids[i] == -1):\n",
    "                        #print(\"Before\",i,bin_id,y_bin_ids[i])\n",
    "                        y_bin_ids[i] = self.nbins_\n",
    "                        x_bin_ids[i] = 0\n",
    "                        #print(\"After\",i,x_bin_ids[i],y_bin_ids[i])\n",
    "                return x_bin_ids, y_bin_ids\n",
    "            else:\n",
    "                return x_bin_ids, y_bin_ids\n",
    "        def dissimilarity(x_bin_ids, y_bin_ids):\n",
    "            len_x, len_y = len(x_bin_ids), len(y_bin_ids)\n",
    "\n",
    "            # check the vector size\n",
    "            if (len_x != self.ndim_) or (len_y != self.ndim_):\n",
    "                raise IndexError(\"Number of columns does not match.\")\n",
    "\n",
    "            m_dissim = self.bin_dissimilarities_[self.dimVec_, x_bin_ids.astype(int), y_bin_ids.astype(int)]\n",
    "            return np.sum(m_dissim) / self.ndim_\n",
    "\n",
    "        \n",
    "\n",
    "        if test is None:\n",
    "            d = np.empty((len(train), len(train)))\n",
    "            x_x = [0.0 for i in range(len(train))]\n",
    "            x_xi = [0.0 for i in range(len(train))]\n",
    "            x_xj = [0.0 for i in range(len(train))]\n",
    "\n",
    "            for i in range(len(train)):\n",
    "                for j in range(i, len(train)):\n",
    "                    train[i], train[j] = convert(train[i], train[j]) \n",
    "                    # updated i and j\n",
    "                    x_y = dissimilarity(train[i], train[j])\n",
    "                    x_xi[i] = dissimilarity(train[i], train[i])\n",
    "                    x_xj[j] = dissimilarity(train[j], train[j])\n",
    "\n",
    "                    d[i][j] = (2.0 * x_y) / (x_xi[i] + x_xj[j])\n",
    "                    d[j][i] = d[i][j]\n",
    "        else:\n",
    "            d = np.empty((len(train), len(test)))\n",
    "            y_y = [0.0 for i in range(len(test))]\n",
    "\n",
    "            for i in range(len(train)):\n",
    "                for j in range(len(test)):\n",
    "                    train[i], test[j] = convert(train[i], test[j])\n",
    "                    x_x = dissimilarity(train[i], train[i])\n",
    "                    y_y[j] = dissimilarity(test[j], test[j])\n",
    "\n",
    "                    x_y = dissimilarity(train[i], test[j])\n",
    "\n",
    "                    d[i][j] = (2.0 * x_y) / (x_x + y_y[j])\n",
    "\n",
    "        return np.array(d)\n",
    "    \n",
    "    def transform_keep(self, train, test=None):\n",
    "        def dissimilarity(x_bin_ids, y_bin_ids):\n",
    "            len_x, len_y = len(x_bin_ids), len(y_bin_ids)\n",
    "\n",
    "            # check the vector size\n",
    "            if (len_x != self.ndim_) or (len_y != self.ndim_):\n",
    "                raise IndexError(\"Number of columns does not match.\")\n",
    "\n",
    "            m_dissim = self.bin_dissimilarities_[self.dimVec_, x_bin_ids.astype(int), y_bin_ids.astype(int)]\n",
    "            return np.sum(m_dissim) / self.ndim_\n",
    "\n",
    "        if test is None:\n",
    "            d = np.empty((len(train), len(train)))\n",
    "            x_x = [0.0 for i in range(len(train))]\n",
    "\n",
    "            for i in range(len(train)):\n",
    "                x_x[i] = dissimilarity(train[i], train[i])\n",
    "\n",
    "            for i in range(len(train)):\n",
    "                for j in range(i, len(train)):\n",
    "                    x_y = dissimilarity(train[i], train[j])\n",
    "\n",
    "                    d[i][j] = (2.0 * x_y) / (x_x[i] + x_x[j])\n",
    "                    d[j][i] = d[i][j]\n",
    "        else:\n",
    "            d = np.empty((len(train), len(test)))\n",
    "            y_y = [0.0 for i in range(len(test))]\n",
    "\n",
    "            for i in range(len(test)):\n",
    "                y_y[i] = dissimilarity(test[i], test[i])\n",
    "\n",
    "            for i in range(len(train)):\n",
    "                x_x = dissimilarity(train[i], train[i])\n",
    "\n",
    "                for j in range(len(test)):\n",
    "                    x_y = dissimilarity(train[i], test[j])\n",
    "\n",
    "                    d[i][j] = (2.0 * x_y) / (x_x + y_y[j])\n",
    "\n",
    "        return np.array(d)\n",
    "    \n",
    "\n",
    "    def transform_ordinal(self, train, test=None):\n",
    "        def dissimilarity(x_bin_ids, y_bin_ids):\n",
    "            len_x, len_y = len(x_bin_ids), len(y_bin_ids)\n",
    "\n",
    "            # check the vector size\n",
    "            if (len_x != self.ndim_) or (len_y != self.ndim_):\n",
    "                raise IndexError(\"Number of columns does not match.\")\n",
    "\n",
    "            m_dissim = self.bin_dissimilarities_[self.dimVec_, x_bin_ids.astype(int), y_bin_ids.astype(int)]\n",
    "            return np.sum(m_dissim) / self.ndim_\n",
    "\n",
    "        if test is None:\n",
    "            d = np.empty((len(train), len(train)))\n",
    "            x_x = [0.0 for i in range(len(train))]\n",
    "\n",
    "            for i in range(len(train)):\n",
    "                x_x[i] = dissimilarity(train[i], train[i])\n",
    "\n",
    "            for i in range(len(train)):\n",
    "                for j in range(i, len(train)):\n",
    "                    x_y = dissimilarity(train[i], train[j])\n",
    "\n",
    "                    d[i][j] = (2.0 * x_y) / (x_x[i] + x_x[j])\n",
    "                    d[j][i] = d[i][j]\n",
    "        else:\n",
    "            d = np.empty((len(train), len(test)))\n",
    "            y_y = [0.0 for i in range(len(test))]\n",
    "\n",
    "            for i in range(len(test)):\n",
    "                y_y[i] = dissimilarity(test[i], test[i])\n",
    "\n",
    "            for i in range(len(train)):\n",
    "                x_x = dissimilarity(train[i], train[i])\n",
    "\n",
    "                for j in range(len(test)):\n",
    "                    x_y = dissimilarity(train[i], test[j])\n",
    "\n",
    "                    d[i][j] = (2.0 * x_y) / (x_x + y_y[j])\n",
    "\n",
    "        return np.array(d)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ctypes import c_float\n",
    "\n",
    "import numpy as np\n",
    "from bisect import bisect_left\n",
    "\n",
    "\n",
    "class EqualFrequencyDiscretizer(object):\n",
    "\n",
    "  def __init__(self, data, nbins, stats):\n",
    "    self.stats = stats\n",
    "    self.n_data = len(data)\n",
    "    self.n_dim = len(data[0])\n",
    "    self.bin_cuts = [[] for i in range(self.n_dim)]\n",
    "    self.bin_counts = [[] for i in range(self.n_dim)]\n",
    "    self.data_bin_ids = np.array([[-1 for i in range(self.n_dim)] for i in range(self.n_data)])\n",
    "    # initialized with -1\n",
    "    self.num_bins = [0 for i in range(self.n_dim)]\n",
    "    for i in range(self.n_dim):\n",
    "      if (self.stats is None) or (\"Numeric\" in self.stats[\"attribute\"][i][\"type\"]):\n",
    "\n",
    "        column_data = data[:, i] \n",
    "        # looking at each column\n",
    "        # Filter out NaN values from the column\n",
    "        temp = column_data[~np.isnan(column_data)]\n",
    "        b_cuts, b_counts = self.equal_freq_histograms(temp, nbins)\n",
    "        #b_cuts, b_counts = self.equal_freq_histograms(data[:, i], nbins)\n",
    "        # b_cuts, b_counts = self.equal_freq_histograms_weka(data[:,i], nbins)\n",
    "      else:\n",
    "        #non-numerical\n",
    "        column_data = data[:, i] \n",
    "        # looking at each column\n",
    "        # Filter out NaN values from the column\n",
    "        temp = column_data[column_data != -1]\n",
    "        b_cuts, b_counts = self.equal_freq_histograms_non_numeric(temp, i)\n",
    "        #b_cuts, b_counts = self.equal_freq_histograms_non_numeric(data[:, i], i)\n",
    "\n",
    "      self.bin_cuts[i] = b_cuts\n",
    "      self.bin_counts[i] = b_counts\n",
    "      self.num_bins[i] = len(b_counts)\n",
    "      for j in range(self.n_data):\n",
    "        # for each column, look at each item\n",
    "        if (self.stats is None) or (\"Numeric\" in self.stats[\"attribute\"][i][\"type\"]):\n",
    "            if np.isnan(data[j, i]):\n",
    "              # fill in the bin-id into -1 to indicate the missing\n",
    "              self.data_bin_ids[j,i] = -1\n",
    "            else:\n",
    "              self.data_bin_ids[j,i] = bisect_left(b_cuts[1:-1], data[j,i])\n",
    "        else:\n",
    "            if (data[j, i]) == -1:\n",
    "              self.data_bin_ids[j,i] = -1\n",
    "            else:\n",
    "              self.data_bin_ids[j,i] = int(data[j,i])\n",
    "  def get_bin_cuts_counts(self):\n",
    "    return self.bin_cuts, self.bin_counts\n",
    "\n",
    "  def get_num_bins(self):\n",
    "    return self.num_bins \n",
    "\n",
    "  def get_data_bin_id(self):\n",
    "    return np.array(self.data_bin_ids, dtype = c_float)\n",
    "\n",
    "  def get_bin_id(self, x):\n",
    "    x_bin_ids = [-1 for i in range(self.n_dim)]\n",
    "    for i in range(self.n_dim):\n",
    "      if (self.stats is None) or (\"Numeric\" in self.stats[\"attribute\"][i][\"type\"]):\n",
    "        if np.isnan(x[i]):\n",
    "              x_bin_ids[i] = -1\n",
    "        else:\n",
    "          cuts = self.bin_cuts[i]\n",
    "          x_bin_ids[i] = bisect_left(cuts[1:-1], x[i])\n",
    "      else:\n",
    "        x_bin_ids[i] = int(x[i])\n",
    "\n",
    "    return np.array(x_bin_ids)\n",
    "\n",
    "  def equal_freq_histograms_non_numeric(self, x, idx):\n",
    "    # get unique values and counts\n",
    "    unique_values, unique_value_counts = np.unique(x, return_counts = True)\n",
    "\n",
    "    if (self.stats is not None) and (\"Numeric\" not in self.stats[\"attribute\"][idx][\"type\"]):\n",
    "      chk_cnt = []\n",
    "      idx_chk = 0\n",
    "\n",
    "      for i in range(len(self.stats[\"attribute\"][idx][\"values\"])):\n",
    "        if (idx_chk < len(unique_values)) and (unique_values[idx_chk] == i):\n",
    "          chk_cnt.append(unique_value_counts[idx_chk])\n",
    "          idx_chk += 1\n",
    "        else:\n",
    "          chk_cnt.append(0)\n",
    "\n",
    "      unique_value_counts = chk_cnt\n",
    "\n",
    "    # return the result\n",
    "    return np.array([]), np.array(unique_value_counts)\n",
    "\n",
    "\n",
    "  def equal_freq_histograms(self, x, nbins):\n",
    "\n",
    "    b_cuts = []\n",
    "    b_counts = []\n",
    "\n",
    "    # get unique values and counts\n",
    "    unique_values, unique_value_counts = np.unique(x, return_counts=True)\n",
    "    num_unique_vals = len(unique_values)\n",
    "\n",
    "    # start discretization\n",
    "    x_size = len(x)\n",
    "    exp_freq = x_size/nbins\n",
    "    freq_count = 0\n",
    "    last_freq_count = 0\n",
    "    last_id = -1\n",
    "    cut_point_id = 0\n",
    "\n",
    "    b_cuts.append(unique_values[0] - (unique_values[1] - unique_values[0]) / 2)\n",
    "\n",
    "    for i in range(num_unique_vals-1):\n",
    "      freq_count += unique_value_counts[i]\n",
    "      x_size -= unique_value_counts[i]\n",
    "      # check if ideal bin count is reached\n",
    "      if (freq_count >= exp_freq):\n",
    "        # check if this one is worst than the last one\n",
    "        if (((exp_freq - last_freq_count) < (freq_count - exp_freq)) and (last_id != -1) ):\n",
    "          cut_point = (unique_values[last_id] + unique_values[last_id+1])/2\n",
    "          # check if it worths merging the about to create bin with the last bin\n",
    "          if (len(b_counts) > 1):\n",
    "            if ((abs(b_counts[-1] + last_freq_count) - exp_freq) < abs(last_freq_count - exp_freq)):\n",
    "              b_counts[-1] += last_freq_count\n",
    "              b_cuts[-1] = cut_point\n",
    "            else: \n",
    "              b_cuts.append(cut_point)\n",
    "              b_counts.append(last_freq_count)\n",
    "          else:\n",
    "              b_cuts.append(cut_point)\n",
    "              b_counts.append(last_freq_count)              \n",
    "          freq_count -= last_freq_count\n",
    "          last_freq_count = freq_count\n",
    "          last_id = i\n",
    "        else:\n",
    "          b_cuts.append((unique_values[i] + unique_values[i+1])/2)\n",
    "          b_counts.append(freq_count)\n",
    "          freq_count = 0\n",
    "          last_freq_count = 0\n",
    "          last_id = -1\n",
    "        # increase the counter\n",
    "        cut_point_id += 1\n",
    "        # exp_freq = (x_size + freq_count) / (nbins - cut_point_id)\n",
    "      else:  \n",
    "        last_id = i\n",
    "        last_freq_count = freq_count\n",
    "\n",
    "    # what to do with the last unique value frequency\n",
    "    last_unique_value_count = unique_value_counts[i+1] \n",
    "    freq_count = freq_count + last_unique_value_count\n",
    "    x_size -= unique_value_counts[i+1]\n",
    "\n",
    "    # Just make sure that it is the last unique value\n",
    "    if (x_size != 0):\n",
    "      print('ERROR: Something is wrong, x_size should be 0 but x_size=%s' % (x_size))\n",
    "      exit()\n",
    "     \n",
    "    # check if the next partition is required\n",
    "    if ((last_id != -1) and (abs(exp_freq - last_unique_value_count) < abs(freq_count - exp_freq))):\n",
    "      b_cuts.append((unique_values[last_id] + unique_values[last_id+1])/2)\n",
    "      b_counts.append(last_freq_count)\n",
    "      freq_count -= last_freq_count\n",
    "\n",
    "    b_counts.append(freq_count)\n",
    "     \n",
    "    # check if the last partition can be merged with the one before\n",
    "    if (len(b_counts) >= 2):    \n",
    "      if (abs((b_counts[-2] + b_counts[-1]) - exp_freq) < abs(exp_freq - b_counts[-1])): \n",
    "         b_counts[-2] += b_counts[-1]\n",
    "         del b_cuts[-1]\n",
    "         del b_counts[-1]\n",
    "\n",
    "    # check if it is worth merging the second last bin with the third last\n",
    "    if (len(b_counts) >= 3):\n",
    "      if (abs((b_counts[-3] + b_counts[-2]) - exp_freq) < abs(exp_freq - b_counts[-2])):\n",
    "        b_counts[-3] += b_counts[-2]\n",
    "        b_counts[-2] = b_counts[-1]\n",
    "        del b_cuts[-2]\n",
    "        del b_counts[-1]\n",
    "\n",
    "    b_cuts.append(unique_values[num_unique_vals-1] + (unique_values[num_unique_vals-1] - unique_values[num_unique_vals-2]) / 2) \n",
    "\n",
    "    assert sum(b_counts) == len(x)\n",
    "    assert len(b_cuts) == (len(b_counts) + 1)\n",
    "\n",
    "    # return the result\n",
    "    return np.array(b_cuts), np.array(b_counts)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_missing(data, missing_rate = 0.2):\n",
    "    total_elements = data.size\n",
    "    missing_elements = int(total_elements * missing_rate)\n",
    "\n",
    "    # Create a random mask\n",
    "    mask_indices = np.random.choice(total_elements, missing_elements, replace=False)\n",
    "\n",
    "    # Convert flat indices to multi-dimensional indices\n",
    "    multi_indices = np.unravel_index(mask_indices, data.shape)\n",
    "\n",
    "    # Set selected elements to NaN\n",
    "    data[multi_indices] = np.nan\n",
    "\n",
    "    return data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
