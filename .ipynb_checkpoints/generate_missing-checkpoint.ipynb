{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from ucimlrepo import fetch_ucirepo \n",
    "import pandas as pd\n",
    "import json\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataname_list = [\"banknote\",\"yeast\",\"climate_model_crashes\",\n",
    "                 \"wine_quality_white\", \"yacht_hydrodynamics\",\"concrete_compression\",\n",
    "                 \"breast_cancer\",\"solar_fire\",\"car_evluation\"\n",
    "                 ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_split_index_cv(scaled_data, directory_path, seed=1, nfold=5):\n",
    "    indlist = np.arange(len(scaled_data))\n",
    "\n",
    "    np.random.seed(seed)\n",
    "    np.random.shuffle(indlist)\n",
    "\n",
    "    fold_size = len(scaled_data) // nfold\n",
    "    save_index = {}\n",
    "\n",
    "    for fold in range(nfold):\n",
    "        start = fold * fold_size\n",
    "        end = start + fold_size if fold < nfold - 1 else len(scaled_data)\n",
    "        \n",
    "        test_index = indlist[start:end]\n",
    "        train_index = np.concatenate([indlist[:start], indlist[end:]])\n",
    "\n",
    "        # If you want to split the training set into train and validation sets\n",
    "        num_train = int(len(train_index) * 0.9)\n",
    "        train_subindex = train_index[:num_train]\n",
    "        valid_subindex = train_index[num_train:]\n",
    "\n",
    "        fold_index = {\n",
    "            \"test_index\": test_index.astype(np.int64).tolist(),\n",
    "            \"train_index\": train_subindex.astype(np.int64).tolist(),\n",
    "            \"valid_index\": valid_subindex.astype(np.int64).tolist()\n",
    "        }\n",
    "        save_index[f\"fold_{fold+1}\"] = fold_index\n",
    "\n",
    "    with open(f\"data/{directory_path}/split_index_cv_seed-{seed}_nfold-{nfold}.json\", 'w') as file:\n",
    "        json.dump(save_index, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(name):\n",
    "    if name == \"banknote\":\n",
    "        with open('data/banknote/data_banknote_authentication.txt', 'rb') as f:\n",
    "            df = pd.read_csv(f, low_memory=False, sep=',',header = None)\n",
    "            Xy = {}\n",
    "            # Ignore the two blocking factor\n",
    "            Xy['data'] = df.values[:, :-1]\n",
    "            Xy['target'] =  df.values[:, -1]\n",
    "    elif name == \"yeast\":\n",
    "        with open('data/yeast/yeast.data', 'rb') as f:\n",
    "            df = pd.read_csv(f, delimiter='\\s+', header = None)\n",
    "            Xy = {}\n",
    "            # remove index\n",
    "            Xy['data'] = df.values[:, 1:-1].astype('float')\n",
    "            Xy['target'] =  df.values[:, -1]\n",
    "    elif name == \"climate_model_crashes\":\n",
    "        with open('data/climate_model_crashes/pop_failures.dat', 'rb') as f:\n",
    "            df = pd.read_csv(f, delimiter='\\s+', header = 0)\n",
    "            Xy = {}\n",
    "            # Ignore the two blocking factor\n",
    "            Xy['data'] = df.values[:, 2:-1]\n",
    "            Xy['target'] =  df.values[:, -1]\n",
    "    elif name == \"wine_quality_white\":\n",
    "        with open('data/wine_quality_white/data.csv', 'rb') as f:\n",
    "            df = pd.read_csv(f, delimiter=';')\n",
    "            Xy = {}\n",
    "            Xy['data'] = df.values[:, :-1].astype('float')\n",
    "            Xy['target'] =  df.values[:, -1]\n",
    "\n",
    "    elif name == \"yacht_hydrodynamics\":\n",
    "        with open('data/yacht_hydrodynamics/yacht_hydrodynamics.data', 'rb') as f:\n",
    "            df = pd.read_csv(f, delimiter='\\s+', header = None)\n",
    "            Xy = {}\n",
    "            Xy['data'] = df.values[:, :-1]\n",
    "            Xy['target'] =  df.values[:, -1]\n",
    "\n",
    "    elif name == \"concrete_compression\":\n",
    "        with open('data/concrete_compression/Concrete_Data.xls', 'rb') as f:\n",
    "            df = pd.read_excel(io=f)\n",
    "            Xy = {}\n",
    "            Xy['data'] = df.values[:, :-1]\n",
    "            Xy['target'] =  df.values[:, -1]\n",
    "\n",
    "    elif name == \"breast_cancer\":\n",
    "        with open('data/breast_cancer/breast_cancer.data', 'rb') as f:\n",
    "            df = pd.read_csv(f, delimiter=',', header = None)\n",
    "            Xy = {}\n",
    "            Xy['data'] = df.values[:, :-1]\n",
    "            Xy['target'] =  df.values[:, -1]\n",
    "\n",
    "    elif name == \"solar_fire\":\n",
    "        with open('data/solar_fire/flare.data1', 'rb') as f:\n",
    "            df1 = pd.read_csv(f, delimiter='\\s+', header = None)\n",
    "        with open('data/solar_fire/flare.data2', 'rb') as f:\n",
    "            df2 = pd.read_csv(f, delimiter='\\s+', header = None)\n",
    "            df = pd.concat([df1, df2], ignore_index=True)\n",
    "            Xy = {}\n",
    "            Xy['data'] = df.values[:, :-1]\n",
    "            Xy['target'] =  df.values[:, -1]\n",
    "\n",
    "    elif name == \"car_evaluation\":\n",
    "        with open('data/car_evaluation/car.data', 'rb') as f:\n",
    "            df = pd.read_csv(f, delimiter=',', header = None)\n",
    "            Xy = {}\n",
    "            Xy['data'] = df.values[:, :-1]\n",
    "            Xy['target'] =  df.values[:, -1]\n",
    "\n",
    "    return Xy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "banknote\n",
      "[0.5        0.53571429 0.5393586  0.35058309] 0.4814139941690962\n",
      "\n",
      "yeast\n",
      "[0.66442049 0.35377358 0.71698113 0.29716981 0.17318059 0.98382749\n",
      " 0.22506739 0.74326146] 0.5197102425876011\n",
      "\n",
      "climate_model_crashes\n",
      "[0.48333333 0.5        0.50925926 0.47592593 0.46851852 0.52407407\n",
      " 0.48518519 0.4962963  0.50555556 0.50555556 0.48888889 0.53888889\n",
      " 0.50185185 0.48333333 0.52777778 0.47407407 0.52777778 0.51851852] 0.5008230452674897\n",
      "\n",
      "wine_quality_white\n",
      "0.02927946078051824 0.5\n",
      "[0.98734177 0.         1.         0.48101266 1.         0.21314822\n",
      " 0.92935892 1.         0.86729277 1.         0.99612087] 0.7703886558521104\n",
      "\n",
      "yacht_hydrodynamics\n",
      "[0.94805195 1.         1.         0.0487013  0.00974026 0.        ] 0.5010822510822511\n",
      "\n",
      "concrete_compression\n",
      "[0.65728155 0.71747573 0.19223301 0.7592233  1.         0.\n",
      " 0.         0.84563107] 0.5214805825242719\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataname_list = [\"banknote\",\"yeast\",\"climate_model_crashes\",\n",
    "                 \"wine_quality_white\", \n",
    "                 \"yacht_hydrodynamics\",\n",
    "                 \"concrete_compression\",\n",
    "                #  \"breast_cancer\",\n",
    "                #  \"solar_fire\",\n",
    "                #  \"car_evaluation\"\n",
    "                 ]\n",
    "save = False\n",
    "p = 0.5 \n",
    "\n",
    "missing_rate = [0.05,0.1,0.3]\n",
    "\n",
    "\n",
    "#dataname_list = [\"yeast\"]\n",
    "for name in dataname_list:\n",
    "    print(name)\n",
    "    Xy = load_data(name)\n",
    "    \n",
    "    feature = Xy['data']\n",
    "    label = Xy['target']\n",
    "\n",
    "    #MCAR_mask = MCAR(feature, p, seed=1)\n",
    "    # print(\"MAR\")\n",
    "    #MAR_mask = MAR(feature, p)\n",
    "    #print(\"MNAR\")\n",
    "    MNAR_mask = MNAR(feature, p)\n",
    "    \n",
    "    \n",
    "\n",
    "    if save:\n",
    "        save_split_index_cv(Xy['data'],name,seed = 1,nfold = 5)\n",
    "        np.save(f\"data/{name}/feature.npy\", Xy['data'])\n",
    "        np.save(f\"data/{name}/label.npy\", Xy['target'])\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Missing Mechan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MCAR(observed_values, p, seed=1):\n",
    "    np.random.seed(seed)\n",
    "    num_rows, num_cols = observed_values.shape\n",
    "    num_to_remove_per_column = int(num_rows * p)\n",
    "    masks = np.ones_like(observed_values)\n",
    "    for col in range(num_cols):\n",
    "        indices_to_remove = np.random.choice(num_rows, num_to_remove_per_column, replace=False)\n",
    "        masks[indices_to_remove, col] = 0\n",
    "    calculate_missing_rates(masks)\n",
    "    return masks\n",
    "\n",
    "def force_mask(mask, p, tolerance=0.01,seed = 1):\n",
    "    rows, cols = mask.shape\n",
    "    \n",
    "    for col in range(cols):\n",
    "        missing_rate = np.mean(mask[:, col] == 0)\n",
    "        \n",
    "        while abs(missing_rate - p) > tolerance:\n",
    "            if missing_rate < p:\n",
    "                # Need more zeros, randomly change 1s to 0s\n",
    "                one_indices = np.where(mask[:, col] == 1)[0]\n",
    "                if len(one_indices) == 0:\n",
    "                    break  # No more 1s to change to 0s\n",
    "                np.random.seed(seed)\n",
    "                random_index = np.random.choice(one_indices)\n",
    "                mask[random_index, col] = 0\n",
    "            else:\n",
    "                # Need more ones, randomly change 0s to 1s\n",
    "                zero_indices = np.where(mask[:, col] == 0)[0]\n",
    "                if len(zero_indices) == 0:\n",
    "                    break  # No more 0s to change to 1s\n",
    "                np.random.seed(seed)\n",
    "                random_index = np.random.choice(zero_indices)\n",
    "                mask[random_index, col] = 1\n",
    "\n",
    "            missing_rate = np.mean(mask[:, col] == 0)\n",
    "    \n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_missing_rates(mask):\n",
    "    # Calculate missing rate for each column\n",
    "    num_rows, num_cols = mask.shape\n",
    "    missing_rate_per_column = np.sum(mask == 0, axis=0) / num_rows\n",
    "\n",
    "    # Calculate overall missing rate\n",
    "    total_elements = num_rows * num_cols\n",
    "    overall_missing_rate = np.sum(mask == 0) / total_elements\n",
    "\n",
    "    print(missing_rate_per_column, overall_missing_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from scipy import optimize\n",
    "\n",
    "def pick_coeffs(X, idxs_obs=None, idxs_nas=None, self_mask=False):\n",
    "    n, d = X.shape\n",
    "    if self_mask:\n",
    "        torch.manual_seed(d)\n",
    "        coeffs = torch.randn(d)\n",
    "        Wx = X * coeffs\n",
    "        coeffs /= torch.std(Wx, 0)\n",
    "    else:\n",
    "        d_obs = len(idxs_obs)\n",
    "        d_na = len(idxs_nas)\n",
    "        torch.manual_seed(d)\n",
    "        coeffs = torch.randn(d_obs, d_na).float()\n",
    "\n",
    "        # Dynamically adjust coeffs to match the type of X[:, idxs_obs]\n",
    "        if X[:, idxs_obs].dtype == torch.double:\n",
    "            coeffs = coeffs.double()\n",
    "        # Add more conditions here if there are other types you need to handle\n",
    "\n",
    "        # Perform operations\n",
    "        Wx = X[:, idxs_obs].mm(coeffs)\n",
    "        coeffs /= torch.std(Wx, 0, keepdim=True)\n",
    "    return coeffs\n",
    "\n",
    "\n",
    "def fit_intercepts(X, coeffs, p, self_mask=False):\n",
    "    if self_mask:\n",
    "        d = len(coeffs)\n",
    "        intercepts = torch.zeros(d)\n",
    "        for j in range(d):\n",
    "            def f(x):\n",
    "                return torch.sigmoid(X * coeffs[j] + x).mean().item() - p\n",
    "            \n",
    "            try:\n",
    "                intercepts[j] = optimize.bisect(f, -500, 500)\n",
    "            except:\n",
    "                print(f(-500),f(500))\n",
    "    else:\n",
    "        d_obs, d_na = coeffs.shape\n",
    "        intercepts = torch.zeros(d_na)\n",
    "        for j in range(d_na):\n",
    "            def f(x):\n",
    "                return torch.sigmoid(X.mv(coeffs[:, j]) + x).mean().item() - p\n",
    "            #intercepts[j] = optimize.bisect(f, -500, 500)\n",
    "            try:\n",
    "                intercepts[j] = optimize.bisect(f, -500, 500)\n",
    "            except:\n",
    "                print(f(-500),f(500))\n",
    "            \n",
    "    return intercepts\n",
    "\n",
    "\n",
    "def MAR(X, p, p_obs = 0.5,seed = 1):\n",
    "\n",
    "    n, d = X.shape\n",
    "\n",
    "    to_torch = torch.is_tensor(X) ## output a pytorch tensor, or a numpy array\n",
    "    if not to_torch:\n",
    "        X = torch.from_numpy(X)\n",
    "\n",
    "    mask = torch.zeros(n, d).bool() if to_torch else np.zeros((n, d)).astype(bool)\n",
    "\n",
    "    d_obs = max(int(p_obs * d), 1) ## number of variables that will have no missing values (at least one variable)\n",
    "    d_na = d - d_obs ## number of variables that will have missing values\n",
    "\n",
    "    ### Sample variables that will all be observed, and those with missing values:\n",
    "    np.random.seed(n)\n",
    "    idxs_obs = np.random.choice(d, d_obs, replace=False)\n",
    "    idxs_nas = np.array([i for i in range(d) if i not in idxs_obs])\n",
    "\n",
    "    ### Other variables will have NA proportions that depend on those observed variables, through a logistic model\n",
    "    ### The parameters of this logistic model are random.\n",
    "\n",
    "    ### Pick coefficients so that W^Tx has unit variance (avoids shrinking)\n",
    "    coeffs = pick_coeffs(X, idxs_obs, idxs_nas)\n",
    "    ### Pick the intercepts to have a desired amount of missing values\n",
    "    intercepts = fit_intercepts(X[:, idxs_obs], coeffs, p)\n",
    "\n",
    "    ps = torch.sigmoid(X[:, idxs_obs].mm(coeffs) + intercepts)\n",
    "    torch.manual_seed(n)\n",
    "    ber = torch.rand(n, d_na)\n",
    "    mask[:, idxs_nas] = ber >= ps\n",
    "    #mask = force_mask(mask,p)\n",
    "    calculate_missing_rates(mask)\n",
    "    return mask\n",
    "\n",
    "#\n",
    "\n",
    "def MNAR(X, p):\n",
    "\n",
    "    n, d = X.shape\n",
    "\n",
    "    to_torch = torch.is_tensor(X) ## output a pytorch tensor, or a numpy array\n",
    "    if not to_torch:\n",
    "        X = torch.from_numpy(X)\n",
    "\n",
    "    ### Variables will have NA proportions that depend on those observed variables, through a logistic model\n",
    "    ### The parameters of this logistic model are random.\n",
    "\n",
    "    ### Pick coefficients so that W^Tx has unit variance (avoids shrinking)\n",
    "    coeffs = pick_coeffs(X, self_mask=True)\n",
    "    \n",
    "    ### Pick the intercepts to have a desired amount of missing values\n",
    "    intercepts = fit_intercepts(X, coeffs, p, self_mask=True)\n",
    "    \n",
    "\n",
    "    ps = torch.sigmoid(X * coeffs + intercepts)\n",
    "\n",
    "\n",
    "    np.random.seed(n)\n",
    "    ber = np.random.rand(n, d)\n",
    "    mask = ber >= ps if to_torch else ber >= ps.numpy()\n",
    "    #mask = force_mask(mask,p)\n",
    "    \n",
    "    calculate_missing_rates(mask)\n",
    "    return mask\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
